{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    getMasterOutout,\n",
    "    scale_intrinsics,\n",
    "    CameraMatrix,\n",
    "    run_pnp,\n",
    "    getImageFromIndex,\n",
    ")\n",
    "\n",
    "master_size = [512,384] #size of image used by mast3r\n",
    "\n",
    "#imports for visualizing matches\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as pl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import cv2 #for pnp\n",
    "from pyproj import Proj, transform #cartographic transformations and coordinate conversions\n",
    "\n",
    "#supressing unnecessary warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import torch\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "import json\n",
    "#ensuring i dont exceed cpu limits?\n",
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "def camera_b_to_a(camera_a_to_world, camera_b_to_world):\n",
    "    #Inverse of camera A to world\n",
    "    world_to_camera_a = np.linalg.inv(camera_a_to_world)\n",
    "    \n",
    "    # Camera B to A = (A to World)^-1 * (B to World)\n",
    "    camera_b_to_a = np.dot(world_to_camera_a, camera_b_to_world)\n",
    "    \n",
    "    return camera_b_to_a\n",
    "\n",
    "#load model\n",
    "device = 'cuda:1'\n",
    "model_name = \"naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric\"\n",
    "model = AsymmetricMASt3R.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sevenScenesDatasets import SevenScenesNBDataset\n",
    "root_dir = '/datasets/7scenes_org'\n",
    "pairs_file = '/home/bjangley/VPR/7scenes/pairs2/test_tuples_multiimagerelposenet.txt'\n",
    "output_file = '/home/bjangley/VPR/7scenes/pairs2/results_dud.txt'\n",
    "output_file2 = '/home/bjangley/VPR/7scenes/pairs2/results_n30_v1.txt'\n",
    "dataset = SevenScenesNBDataset(root_dir, pairs_file, mast3r_output=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = dataset[4233]\n",
    "print(item['query_path'])\n",
    "print(len(dataset))\n",
    "print(item['mast3r_query_pose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "K = CameraMatrix(585,585,320,240)\n",
    "n_matches = 30\n",
    "output_file = '/home/bjangley/VPR/7scenes/pairs2/results_n30_v1.txt'\n",
    "# fails_file = '/home/bjangley/VPR/7scenes/pairs2/fails.json'\n",
    "total_items = len(dataset)\n",
    "\n",
    "fails = {}\n",
    "start_index = 4234 #one minus the index it's going to write to\n",
    "mode = 'w' if start_index == 0 else 'a'\n",
    "\n",
    "with open(output_file, mode) as f:\n",
    "    pbar = tqdm(total=total_items-start_index, desc=\"Processing dataset\",initial=start_index)\n",
    "    for i in range(start_index,total_items):\n",
    "        neighbourhood = dataset[i]\n",
    "        query_path = neighbourhood['query_path']\n",
    "        f.write(f\"{query_path}\")\n",
    "        w,h = Image.open(query_path).convert('RGB').size\n",
    "        K_scaled = scale_intrinsics(K,w,h, master_size[0],master_size[1])\n",
    "        neighbourhood_fails = []\n",
    "        for index,anchor_path in enumerate(neighbourhood['anchors_path']):\n",
    "            filtered_matches_im0,filtered_matches_im1,matches_im0, matches_im1, pts3d_im0, pts3d_im1, conf_im0, conf_im1, desc_conf_im0, desc_conf_im1 = getMasterOutout(model, device, anchor_path, neighbourhood['query_path'], n_matches,visualizeMatches=False,verboseFlag=False)\n",
    "\n",
    "            ret_val, transformation = run_pnp(filtered_matches_im1.astype(np.float32), pts3d_im0[filtered_matches_im0[:, 1], filtered_matches_im0[:, 0], :].astype(np.float32), K_scaled.astype(np.float32))\n",
    "            if ret_val:\n",
    "                mast3r_query_pose =  np.dot(neighbourhood['anchor_poses'][index], transformation)\n",
    "                f.write(f\" {' '.join(map(str, transformation.flatten()))}\")\n",
    "                f.write(f\" {' '.join(map(str, mast3r_query_pose.flatten()))}\")\n",
    "            else:\n",
    "                neighbourhood_fails.append(index)\n",
    "                f.write(\" \" + \" \".join([\"0\"]*32))\n",
    "        f.write(f\"\\n\")\n",
    "        # if neighbourhood_fails:\n",
    "        #     fails[query_path] = neighbourhood_fails\n",
    "        pbar.update(1)  # Update progress bar after each item\n",
    "    pbar.close()\n",
    "\n",
    "# with open(fails_file, 'w') as f:\n",
    "#     json.dump(fails, f, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtxt(results_file):\n",
    "    results = []\n",
    "    with open(results_file, 'r') as f:\n",
    "        for line in f:\n",
    "            chunks = line.strip().split(' ')\n",
    "            query_path = chunks[0]\n",
    "            if len(chunks) !=89:\n",
    "                print(\"error\")\n",
    "\n",
    "file = \"/home/bjangley/VPR/7scenes/pairs2/test_tuples_multiimagerelposenet.txt\"\n",
    "readtxt(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_pose_error(est_pose, gt_pose):\n",
    "    # Compute positional error\n",
    "    pos_error = np.linalg.norm(est_pose[:3, 3] - gt_pose[:3, 3])\n",
    "    \n",
    "    est_quat = R.from_matrix(est_pose[:3, :3]).as_quat()\n",
    "    gt_quat = R.from_matrix(gt_pose[:3, :3]).as_quat()\n",
    "    est_quat = np.concatenate(([est_quat[3]], est_quat[:3]))\n",
    "    gt_quat = np.concatenate(([gt_quat[3]], gt_quat[:3]))\n",
    "    # Compute the quaternion dot product and account for double covering.\n",
    "    dot = np.clip(np.abs(np.dot(est_quat, gt_quat)), -1.0, 1.0)\n",
    "    theta = 2 * np.arccos(dot)\n",
    "    \n",
    "    rot_error = np.degrees(theta)\n",
    "\n",
    "    # # Compute rotational error\n",
    "    # R_diff = np.dot(est_pose[:3, :3], gt_pose[:3, :3].T)\n",
    "    # rot_error = np.degrees(np.arccos((np.trace(R_diff) - 1) / 2))\n",
    "    \n",
    "    return pos_error, rot_error\n",
    "\n",
    "\n",
    "def evaluateresults(dataset, results_file, error_threshold):\n",
    "    pos_errors = []\n",
    "    rot_errors = []\n",
    "    large_error_indices = []\n",
    "    \n",
    "    with open(results_file, 'r') as f:\n",
    "        for query_idx, line in enumerate(tqdm(f, desc=\"Evaluating poses\")):\n",
    "            chunks = line.strip().split()\n",
    "            query_path = chunks[0]\n",
    "            \n",
    "            gt_pose = dataset._load_pose(query_path)\n",
    "            \n",
    "            for anchor_idx in range(9):\n",
    "                start_idx = 1 + anchor_idx * 32\n",
    "                end_idx = start_idx + 32\n",
    "                transform_and_pose = chunks[start_idx:end_idx]\n",
    "                \n",
    "                if not all(float(x) == 0 for x in transform_and_pose[16:]):\n",
    "                    mast3r_query_pose = np.array([float(x) for x in transform_and_pose[16:]]).reshape(4, 4)\n",
    "                    pos_error, rot_error = compute_pose_error(mast3r_query_pose, gt_pose)\n",
    "                    pos_errors.append(pos_error)\n",
    "                    rot_errors.append(rot_error)\n",
    "                    \n",
    "                    if pos_error > error_threshold:\n",
    "                        large_error_indices.append((query_idx, anchor_idx))\n",
    "    \n",
    "    return pos_errors, rot_errors, large_error_indices\n",
    "\n",
    "def plot_xz_locations(dataset, results_file, large_error_indices, error_threshold):\n",
    "    gt_x, gt_z = [], []\n",
    "    est_x, est_z = [], []\n",
    "    large_error_x, large_error_z = [], []\n",
    "    \n",
    "    with open(results_file, 'r') as f:\n",
    "        for query_idx, line in enumerate(tqdm(f, desc=\"Processing locations\")):\n",
    "            chunks = line.strip().split()\n",
    "            query_path = chunks[0]\n",
    "            \n",
    "            gt_pose = dataset._load_pose(query_path)\n",
    "            \n",
    "            for anchor_idx in range(9):\n",
    "                start_idx = 1 + anchor_idx * 32\n",
    "                end_idx = start_idx + 32\n",
    "                transform_and_pose = chunks[start_idx:end_idx]\n",
    "                \n",
    "                if not all(float(x) == 0 for x in transform_and_pose[16:]):\n",
    "                    mast3r_query_pose = np.array([float(x) for x in transform_and_pose[16:]]).reshape(4, 4)\n",
    "                    \n",
    "                    if (query_idx, anchor_idx) in large_error_indices:\n",
    "                        large_error_x.append(mast3r_query_pose[0, 3])\n",
    "                        large_error_z.append(mast3r_query_pose[2, 3])\n",
    "                    else:\n",
    "                        est_x.append(mast3r_query_pose[0, 3])\n",
    "                        est_z.append(mast3r_query_pose[2, 3])\n",
    "                    \n",
    "                    gt_x.append(gt_pose[0, 3])\n",
    "                    gt_z.append(gt_pose[2, 3])\n",
    "    \n",
    "    print(f\"Number of queries: {len(gt_x)}\")\n",
    "    print(f\"Number of estimates: {len(est_x) + len(large_error_x)}\")\n",
    "    print(f\"Number of large errors (>{error_threshold}m): {len(large_error_x)}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(gt_x, gt_z, c='blue', label='Ground Truth', alpha=0.5, s=10)\n",
    "    plt.scatter(est_x, est_z, c='green', label=f'Estimated (Error <={error_threshold}m)', alpha=0.5, s=10)\n",
    "    plt.scatter(large_error_x, large_error_z, c='red', label=f'Estimated (Error >{error_threshold}m)', alpha=0.5, s=10)\n",
    "    plt.xlabel('X coordinate')\n",
    "    plt.ylabel('Z coordinate')\n",
    "    plt.title(f'XZ Plot of Query Locations (Error Threshold: {error_threshold}m)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the error threshold\n",
    "error_threshold = 0.25  # You can change this value as needed\n",
    "\n",
    "# Run the evaluation and plotting\n",
    "pos_errors, rot_errors, large_error_indices = evaluateresults(dataset, output_file, error_threshold)\n",
    "\n",
    "# Compute statistics\n",
    "median_pos_error = np.median(pos_errors)\n",
    "median_rot_error = np.median(rot_errors)\n",
    "mean_pos_error = np.mean(pos_errors)\n",
    "mean_rot_error = np.mean(rot_errors)\n",
    "\n",
    "print(f\"Median position error: {median_pos_error:.3f} meters\")\n",
    "print(f\"Median rotation error: {median_rot_error:.3f} degrees\")\n",
    "print(f\"Mean position error: {mean_pos_error:.3f} meters\")\n",
    "print(f\"Mean rotation error: {mean_rot_error:.3f} degrees\")\n",
    "# Create XZ plot\n",
    "plot_xz_locations(dataset, output_file, large_error_indices, error_threshold)\n",
    "print(\"XZ plot displayed\")\n",
    "\n",
    "# Print the list of query and anchor indices with large errors\n",
    "print(f\"\\nQuery and anchor indices with position error > {error_threshold}m:\")\n",
    "for query_idx, anchor_idx in large_error_indices:\n",
    "    print(f\"Query {query_idx}, Anchor {anchor_idx}\")\n",
    "output_file ='/home/bjangley/VPR/7scenes/pairs2/results_dud.txt'\n",
    "# Run the evaluation and plotting\n",
    "pos_errors, rot_errors, large_error_indices = evaluateresults(dataset, output_file, error_threshold)\n",
    "\n",
    "# Compute statistics\n",
    "median_pos_error = np.median(pos_errors)\n",
    "median_rot_error = np.median(rot_errors)\n",
    "mean_pos_error = np.mean(pos_errors)\n",
    "mean_rot_error = np.mean(rot_errors)\n",
    "\n",
    "print(f\"Median position error: {median_pos_error:.3f} meters\")\n",
    "print(f\"Median rotation error: {median_rot_error:.3f} degrees\")\n",
    "print(f\"Mean position error: {mean_pos_error:.3f} meters\")\n",
    "print(f\"Mean rotation error: {mean_rot_error:.3f} degrees\")\n",
    "\n",
    "# Create XZ plot\n",
    "plot_xz_locations(dataset, output_file, large_error_indices, error_threshold)\n",
    "print(\"XZ plot displayed\")\n",
    "\n",
    "# Print the list of query and anchor indices with large errors\n",
    "print(f\"\\nQuery and anchor indices with position error > {error_threshold}m:\")\n",
    "for query_idx, anchor_idx in large_error_indices:\n",
    "    print(f\"Query {query_idx}, Anchor {anchor_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(large_error_indices)\n",
    "print(315*9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx = 3\n",
    "for anchor_idx in range(9):\n",
    "    if (query_idx, anchor_idx) in large_error_indices:\n",
    "        print(f\"Query {query_idx}, Anchor {anchor_idx}: Large error detected (> {error_threshold}m)\")\n",
    "    else:\n",
    "        print(f\"Query {query_idx}, Anchor {anchor_idx}: Error within threshold (<= {error_threshold}m)\")\n",
    "    print(dataset.show_query_and_anchors(query_idx,anchor_idx))\n",
    "\n",
    "query_idx = 3  # The index of the query you're interested in\n",
    "\n",
    "# for anchor_idx in range(9):\n",
    "#     dataset.show_query_and_anchors(query_idx, anchor_idx)\n",
    "    \n",
    "#     print(f\"Checking pair: ({query_idx}, {anchor_idx})\")\n",
    "#     print(f\"Is in large_error_indices: {(query_idx, anchor_idx) in large_error_indices}\")\n",
    "    \n",
    "#     if (query_idx, anchor_idx) in large_error_indices:\n",
    "#         print(f\"Query {query_idx}, Anchor {anchor_idx}: Large error detected (> {error_threshold}m)\")\n",
    "#     else:\n",
    "#         print(f\"Query {query_idx}, Anchor {anchor_idx}: Error within threshold (<= {error_threshold}m)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
