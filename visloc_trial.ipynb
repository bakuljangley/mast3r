{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (C) 2024-present Naver Corporation. All rights reserved.\n",
    "# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n",
    "#\n",
    "# --------------------------------------------------------\n",
    "# visloc script with support for coarse to fine\n",
    "# --------------------------------------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms as tvf\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "from mast3r.fast_nn import fast_reciprocal_NNs\n",
    "from mast3r.utils.coarse_to_fine import select_pairs_of_crops, crop_slice\n",
    "from mast3r.utils.collate import cat_collate, cat_collate_fn_map\n",
    "from mast3r.utils.misc import mkdir_for\n",
    "from mast3r.datasets.utils.cropping import crop_to_homography\n",
    "\n",
    "import mast3r.utils.path_to_dust3r  # noqa\n",
    "from dust3r.inference import inference, loss_of_one_batch\n",
    "from dust3r.utils.geometry import geotrf, colmap_to_opencv_intrinsics, opencv_to_colmap_intrinsics\n",
    "from dust3r.datasets.utils.transforms import ImgNorm\n",
    "from dust3r_visloc.datasets import *\n",
    "from dust3r_visloc.localization import run_pnp\n",
    "from dust3r_visloc.evaluation import get_pose_error, aggregate_stats, export_results\n",
    "from dust3r_visloc.datasets.utils import get_HW_resolution, rescale_points3d\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"visloc dataset to eval\")\n",
    "    parser_weights = parser.add_mutually_exclusive_group(required=True)\n",
    "    parser_weights.add_argument(\"--weights\", type=str, help=\"path to the model weights\", default=None)\n",
    "    parser_weights.add_argument(\"--model_name\", type=str, help=\"name of the model weights\",\n",
    "                                choices=[\"MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric\"])\n",
    "\n",
    "    parser.add_argument(\"--confidence_threshold\", type=float, default=1.001,\n",
    "                        help=\"confidence values higher than threshold are invalid\")\n",
    "    parser.add_argument('--pixel_tol', default=5, type=int)\n",
    "\n",
    "    parser.add_argument(\"--coarse_to_fine\", action='store_true', default=False,\n",
    "                        help=\"do the matching from coarse to fine\")\n",
    "    parser.add_argument(\"--max_image_size\", type=int, default=None,\n",
    "                        help=\"max image size for the fine resolution\")\n",
    "    parser.add_argument(\"--c2f_crop_with_homography\", action='store_true', default=False,\n",
    "                        help=\"when using coarse to fine, crop with homographies to keep cx, cy centered\")\n",
    "\n",
    "    parser.add_argument(\"--device\", type=str, default='cuda', help=\"pytorch device\")\n",
    "    parser.add_argument(\"--pnp_mode\", type=str, default=\"cv2\", choices=['cv2', 'poselib', 'pycolmap'],\n",
    "                        help=\"pnp lib to use\")\n",
    "    parser_reproj = parser.add_mutually_exclusive_group()\n",
    "    parser_reproj.add_argument(\"--reprojection_error\", type=float, default=5.0, help=\"pnp reprojection error\")\n",
    "    parser_reproj.add_argument(\"--reprojection_error_diag_ratio\", type=float, default=None,\n",
    "                               help=\"pnp reprojection error as a ratio of the diagonal of the image\")\n",
    "\n",
    "    parser.add_argument(\"--max_batch_size\", type=int, default=48,\n",
    "                        help=\"max batch size for inference on crops when using coarse to fine\")\n",
    "    parser.add_argument(\"--pnp_max_points\", type=int, default=100_000, help=\"pnp maximum number of points kept\")\n",
    "    parser.add_argument(\"--viz_matches\", type=int, default=0, help=\"debug matches\")\n",
    "\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=None, help=\"output path\")\n",
    "    parser.add_argument(\"--output_label\", type=str, default='', help=\"prefix for results files\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def coarse_matching(query_view, map_view, model, device, pixel_tol, fast_nn_params):\n",
    "    # prepare batch\n",
    "    imgs = []\n",
    "    for idx, img in enumerate([query_view['rgb_rescaled'], map_view['rgb_rescaled']]):\n",
    "        imgs.append(dict(img=img.unsqueeze(0), true_shape=np.int32([img.shape[1:]]),\n",
    "                         idx=idx, instance=str(idx)))\n",
    "    output = inference([tuple(imgs)], model, device, batch_size=1, verbose=False)\n",
    "    pred1, pred2 = output['pred1'], output['pred2']\n",
    "    conf_list = [pred1['desc_conf'].squeeze(0).cpu().numpy(), pred2['desc_conf'].squeeze(0).cpu().numpy()]\n",
    "    desc_list = [pred1['desc'].squeeze(0).detach(), pred2['desc'].squeeze(0).detach()]\n",
    "\n",
    "    # find 2D-2D matches between the two images\n",
    "    PQ, PM = desc_list[0], desc_list[1]\n",
    "    if len(PQ) == 0 or len(PM) == 0:\n",
    "        return [], [], [], []\n",
    "\n",
    "    if pixel_tol == 0:\n",
    "        matches_im_map, matches_im_query = fast_reciprocal_NNs(PM, PQ, subsample_or_initxy1=8, **fast_nn_params)\n",
    "        HM, WM = map_view['rgb_rescaled'].shape[1:]\n",
    "        HQ, WQ = query_view['rgb_rescaled'].shape[1:]\n",
    "        # ignore small border around the edge\n",
    "        valid_matches_map = (matches_im_map[:, 0] >= 3) & (matches_im_map[:, 0] < WM - 3) & (\n",
    "            matches_im_map[:, 1] >= 3) & (matches_im_map[:, 1] < HM - 3)\n",
    "        valid_matches_query = (matches_im_query[:, 0] >= 3) & (matches_im_query[:, 0] < WQ - 3) & (\n",
    "            matches_im_query[:, 1] >= 3) & (matches_im_query[:, 1] < HQ - 3)\n",
    "        valid_matches = valid_matches_map & valid_matches_query\n",
    "        matches_im_map = matches_im_map[valid_matches]\n",
    "        matches_im_query = matches_im_query[valid_matches]\n",
    "        valid_pts3d = []\n",
    "        matches_confs = []\n",
    "    else:\n",
    "        yM, xM = torch.where(map_view['valid_rescaled'])\n",
    "        matches_im_map, matches_im_query = fast_reciprocal_NNs(PM, PQ, (xM, yM), pixel_tol=pixel_tol, **fast_nn_params)\n",
    "        valid_pts3d = map_view['pts3d_rescaled'].cpu().numpy()[matches_im_map[:, 1], matches_im_map[:, 0]]\n",
    "        matches_confs = np.minimum(\n",
    "            conf_list[1][matches_im_map[:, 1], matches_im_map[:, 0]],\n",
    "            conf_list[0][matches_im_query[:, 1], matches_im_query[:, 0]]\n",
    "        )\n",
    "    # from cv2 to colmap\n",
    "    matches_im_query = matches_im_query.astype(np.float64)\n",
    "    matches_im_map = matches_im_map.astype(np.float64)\n",
    "    matches_im_query[:, 0] += 0.5\n",
    "    matches_im_query[:, 1] += 0.5\n",
    "    matches_im_map[:, 0] += 0.5\n",
    "    matches_im_map[:, 1] += 0.5\n",
    "    # rescale coordinates\n",
    "    matches_im_query = geotrf(query_view['to_orig'], matches_im_query, norm=True)\n",
    "    matches_im_map = geotrf(map_view['to_orig'], matches_im_map, norm=True)\n",
    "    # from colmap back to cv2\n",
    "    matches_im_query[:, 0] -= 0.5\n",
    "    matches_im_query[:, 1] -= 0.5\n",
    "    matches_im_map[:, 0] -= 0.5\n",
    "    matches_im_map[:, 1] -= 0.5\n",
    "    return valid_pts3d, matches_im_query, matches_im_map, matches_confs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def crops_inference(pairs, model, device, batch_size=48, verbose=True):\n",
    "    assert len(pairs) == 2, \"Error, data should be a tuple of dicts containing the batch of image pairs\"\n",
    "    # Forward a possibly big bunch of data, by blocks of batch_size\n",
    "    B = pairs[0]['img'].shape[0]\n",
    "    if B < batch_size:\n",
    "        return loss_of_one_batch(pairs, model, None, device=device, symmetrize_batch=False)\n",
    "    preds = []\n",
    "    for ii in range(0, B, batch_size):\n",
    "        sel = slice(ii, ii + min(B - ii, batch_size))\n",
    "        temp_data = [{}, {}]\n",
    "        for di in [0, 1]:\n",
    "            temp_data[di] = {kk: pairs[di][kk][sel]\n",
    "                             for kk in pairs[di].keys() if pairs[di][kk] is not None}  # copy chunk for forward\n",
    "        preds.append(loss_of_one_batch(temp_data, model,\n",
    "                                       None, device=device, symmetrize_batch=False))  # sequential forward\n",
    "    # Merge all preds\n",
    "    return cat_collate(preds, collate_fn_map=cat_collate_fn_map)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def fine_matching(query_views, map_views, model, device, max_batch_size, pixel_tol, fast_nn_params):\n",
    "    assert pixel_tol > 0\n",
    "    output = crops_inference([query_views, map_views],\n",
    "                             model, device, batch_size=max_batch_size, verbose=False)\n",
    "    pred1, pred2 = output['pred1'], output['pred2']\n",
    "    descs1 = pred1['desc'].clone()\n",
    "    descs2 = pred2['desc'].clone()\n",
    "    confs1 = pred1['desc_conf'].clone()\n",
    "    confs2 = pred2['desc_conf'].clone()\n",
    "\n",
    "    # Compute matches\n",
    "    valid_pts3d, matches_im_map, matches_im_query, matches_confs = [], [], [], []\n",
    "    for ppi, (pp1, pp2, cc11, cc21) in enumerate(zip(descs1, descs2, confs1, confs2)):\n",
    "        valid_ppi = map_views['valid'][ppi]\n",
    "        pts3d_ppi = map_views['pts3d'][ppi].cpu().numpy()\n",
    "        conf_list_ppi = [cc11.cpu().numpy(), cc21.cpu().numpy()]\n",
    "\n",
    "        y_ppi, x_ppi = torch.where(valid_ppi)\n",
    "        matches_im_map_ppi, matches_im_query_ppi = fast_reciprocal_NNs(pp2, pp1, (x_ppi, y_ppi),\n",
    "                                                                       pixel_tol=pixel_tol, **fast_nn_params)\n",
    "\n",
    "        valid_pts3d_ppi = pts3d_ppi[matches_im_map_ppi[:, 1], matches_im_map_ppi[:, 0]]\n",
    "        matches_confs_ppi = np.minimum(\n",
    "            conf_list_ppi[1][matches_im_map_ppi[:, 1], matches_im_map_ppi[:, 0]],\n",
    "            conf_list_ppi[0][matches_im_query_ppi[:, 1], matches_im_query_ppi[:, 0]]\n",
    "        )\n",
    "        # inverse operation where we uncrop pixel coordinates\n",
    "        matches_im_map_ppi = geotrf(map_views['to_orig'][ppi].cpu().numpy(), matches_im_map_ppi.copy(), norm=True)\n",
    "        matches_im_query_ppi = geotrf(query_views['to_orig'][ppi].cpu().numpy(), matches_im_query_ppi.copy(), norm=True)\n",
    "\n",
    "        matches_im_map.append(matches_im_map_ppi)\n",
    "        matches_im_query.append(matches_im_query_ppi)\n",
    "        valid_pts3d.append(valid_pts3d_ppi)\n",
    "        matches_confs.append(matches_confs_ppi)\n",
    "\n",
    "    if len(valid_pts3d) == 0:\n",
    "        return [], [], [], []\n",
    "\n",
    "    matches_im_map = np.concatenate(matches_im_map, axis=0)\n",
    "    matches_im_query = np.concatenate(matches_im_query, axis=0)\n",
    "    valid_pts3d = np.concatenate(valid_pts3d, axis=0)\n",
    "    matches_confs = np.concatenate(matches_confs, axis=0)\n",
    "    return valid_pts3d, matches_im_query, matches_im_map, matches_confs\n",
    "\n",
    "\n",
    "def crop(img, mask, pts3d, crop, intrinsics=None):\n",
    "    out_cropped_img = img.clone()\n",
    "    if mask is not None:\n",
    "        out_cropped_mask = mask.clone()\n",
    "    else:\n",
    "        out_cropped_mask = None\n",
    "    if pts3d is not None:\n",
    "        out_cropped_pts3d = pts3d.clone()\n",
    "    else:\n",
    "        out_cropped_pts3d = None\n",
    "    to_orig = torch.eye(3, device=img.device)\n",
    "\n",
    "    # If intrinsics available, crop and apply rectifying homography. Otherwise, just crop\n",
    "    if intrinsics is not None:\n",
    "        K_old = intrinsics\n",
    "        imsize, K_new, R, H = crop_to_homography(K_old, crop)\n",
    "        # apply homography to image\n",
    "        H /= H[2, 2]\n",
    "        homo8 = H.ravel().tolist()[:8]\n",
    "        # From float tensor to uint8 PIL Image\n",
    "        pilim = Image.fromarray((255 * (img + 1.) / 2).to(torch.uint8).numpy())\n",
    "        pilout_cropped_img = pilim.transform(imsize, Image.Transform.PERSPECTIVE,\n",
    "                                             homo8, resample=Image.Resampling.BICUBIC)\n",
    "\n",
    "        # From uint8 PIL Image to float tensor\n",
    "        out_cropped_img = 2. * torch.tensor(np.array(pilout_cropped_img)).to(img) / 255. - 1.\n",
    "        if out_cropped_mask is not None:\n",
    "            pilmask = Image.fromarray((255 * out_cropped_mask).to(torch.uint8).numpy())\n",
    "            pilout_cropped_mask = pilmask.transform(\n",
    "                imsize, Image.Transform.PERSPECTIVE, homo8, resample=Image.Resampling.NEAREST)\n",
    "            out_cropped_mask = torch.from_numpy(np.array(pilout_cropped_mask) > 0).to(out_cropped_mask.dtype)\n",
    "        if out_cropped_pts3d is not None:\n",
    "            out_cropped_pts3d = out_cropped_pts3d.numpy()\n",
    "            out_cropped_X = np.array(Image.fromarray(out_cropped_pts3d[:, :, 0]).transform(imsize,\n",
    "                                                                                           Image.Transform.PERSPECTIVE,\n",
    "                                                                                           homo8,\n",
    "                                                                                           resample=Image.Resampling.NEAREST))\n",
    "            out_cropped_Y = np.array(Image.fromarray(out_cropped_pts3d[:, :, 1]).transform(imsize,\n",
    "                                                                                           Image.Transform.PERSPECTIVE,\n",
    "                                                                                           homo8,\n",
    "                                                                                           resample=Image.Resampling.NEAREST))\n",
    "            out_cropped_Z = np.array(Image.fromarray(out_cropped_pts3d[:, :, 2]).transform(imsize,\n",
    "                                                                                           Image.Transform.PERSPECTIVE,\n",
    "                                                                                           homo8,\n",
    "                                                                                           resample=Image.Resampling.NEAREST))\n",
    "\n",
    "            out_cropped_pts3d = torch.from_numpy(np.stack([out_cropped_X, out_cropped_Y, out_cropped_Z], axis=-1))\n",
    "\n",
    "        to_orig = torch.tensor(H, device=img.device)\n",
    "    else:\n",
    "        out_cropped_img = img[crop_slice(crop)]\n",
    "        if out_cropped_mask is not None:\n",
    "            out_cropped_mask = out_cropped_mask[crop_slice(crop)]\n",
    "        if out_cropped_pts3d is not None:\n",
    "            out_cropped_pts3d = out_cropped_pts3d[crop_slice(crop)]\n",
    "        to_orig[:2, -1] = torch.tensor(crop[:2])\n",
    "\n",
    "    return out_cropped_img, out_cropped_mask, out_cropped_pts3d, to_orig\n",
    "\n",
    "\n",
    "def resize_image_to_max(max_image_size, rgb, K):\n",
    "    W, H = rgb.size\n",
    "    if max_image_size and max(W, H) > max_image_size:\n",
    "        islandscape = (W >= H)\n",
    "        if islandscape:\n",
    "            WMax = max_image_size\n",
    "            HMax = int(H * (WMax / W))\n",
    "        else:\n",
    "            HMax = max_image_size\n",
    "            WMax = int(W * (HMax / H))\n",
    "        resize_op = tvf.Compose([ImgNorm, tvf.Resize(size=[HMax, WMax])])\n",
    "        rgb_tensor = resize_op(rgb).permute(1, 2, 0)\n",
    "        to_orig_max = np.array([[W / WMax, 0, 0],\n",
    "                                [0, H / HMax, 0],\n",
    "                                [0, 0, 1]])\n",
    "        to_resize_max = np.array([[WMax / W, 0, 0],\n",
    "                                  [0, HMax / H, 0],\n",
    "                                  [0, 0, 1]])\n",
    "\n",
    "        # Generate new camera parameters\n",
    "        new_K = opencv_to_colmap_intrinsics(K)\n",
    "        new_K[0, :] *= WMax / W\n",
    "        new_K[1, :] *= HMax / H\n",
    "        new_K = colmap_to_opencv_intrinsics(new_K)\n",
    "    else:\n",
    "        rgb_tensor = ImgNorm(rgb).permute(1, 2, 0)\n",
    "        to_orig_max = np.eye(3)\n",
    "        to_resize_max = np.eye(3)\n",
    "        HMax, WMax = H, W\n",
    "        new_K = K\n",
    "    return rgb_tensor, new_K, to_orig_max, to_resize_max, (HMax, WMax)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_args_parser()\n",
    "    args = parser.parse_args()\n",
    "    conf_thr = args.confidence_threshold\n",
    "    device = args.device\n",
    "    pnp_mode = args.pnp_mode\n",
    "    assert args.pixel_tol > 0\n",
    "    reprojection_error = args.reprojection_error\n",
    "    reprojection_error_diag_ratio = args.reprojection_error_diag_ratio\n",
    "    pnp_max_points = args.pnp_max_points\n",
    "    viz_matches = args.viz_matches\n",
    "\n",
    "    if args.weights is not None:\n",
    "        weights_path = args.weights\n",
    "    else:\n",
    "        weights_path = \"naver/\" + args.model_name\n",
    "    model = AsymmetricMASt3R.from_pretrained(weights_path).to(args.device)\n",
    "    fast_nn_params = dict(device=device, dist='dot', block_size=2**13)\n",
    "    dataset = eval(args.dataset)\n",
    "    dataset.set_resolution(model)\n",
    "\n",
    "    query_names = []\n",
    "    poses_pred = []\n",
    "    pose_errors = []\n",
    "    angular_errors = []\n",
    "    params_str = f'tol_{args.pixel_tol}' + (\"_c2f\" if args.coarse_to_fine else '')\n",
    "    if args.max_image_size is not None:\n",
    "        params_str = params_str + f'_{args.max_image_size}'\n",
    "    if args.coarse_to_fine and args.c2f_crop_with_homography:\n",
    "        params_str = params_str + '_with_homography'\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        views = dataset[(idx)]  # 0 is the query\n",
    "        query_view = views[0]\n",
    "        map_views = views[1:]\n",
    "        query_names.append(query_view['image_name'])\n",
    "\n",
    "        query_pts2d = []\n",
    "        query_pts3d = []\n",
    "        maxdim = max(model.patch_embed.img_size)\n",
    "        query_rgb_tensor, query_K, query_to_orig_max, query_to_resize_max, (HQ, WQ) = resize_image_to_max(\n",
    "            args.max_image_size, query_view['rgb'], query_view['intrinsics'])\n",
    "\n",
    "        # pairs of crops have the same resolution\n",
    "        query_resolution = get_HW_resolution(HQ, WQ, maxdim=maxdim, patchsize=model.patch_embed.patch_size)\n",
    "        for map_view in map_views:\n",
    "            if args.output_dir is not None:\n",
    "                cache_file = os.path.join(args.output_dir, 'matches', params_str,\n",
    "                                          query_view['image_name'], map_view['image_name'] + '.npz')\n",
    "            else:\n",
    "                cache_file = None\n",
    "\n",
    "            if cache_file is not None and os.path.isfile(cache_file):\n",
    "                matches = np.load(cache_file)\n",
    "                valid_pts3d = matches['valid_pts3d']\n",
    "                matches_im_query = matches['matches_im_query']\n",
    "                matches_im_map = matches['matches_im_map']\n",
    "                matches_conf = matches['matches_conf']\n",
    "            else:\n",
    "                # coarse matching\n",
    "                if args.coarse_to_fine and (maxdim < max(WQ, HQ)):\n",
    "                    # use all points\n",
    "                    _, coarse_matches_im0, coarse_matches_im1, _ = coarse_matching(query_view, map_view, model, device,\n",
    "                                                                                   0, fast_nn_params)\n",
    "\n",
    "                    # visualize a few matches\n",
    "                    if viz_matches > 0:\n",
    "                        num_matches = coarse_matches_im1.shape[0]\n",
    "                        print(f'found {num_matches} matches')\n",
    "\n",
    "                        viz_imgs = [np.array(query_view['rgb']), np.array(map_view['rgb'])]\n",
    "                        from matplotlib import pyplot as pl\n",
    "                        n_viz = viz_matches\n",
    "                        match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)\n",
    "                        viz_matches_im_query = coarse_matches_im0[match_idx_to_viz]\n",
    "                        viz_matches_im_map = coarse_matches_im1[match_idx_to_viz]\n",
    "\n",
    "                        H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]\n",
    "                        img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)),\n",
    "                                      'constant', constant_values=0)\n",
    "                        img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)),\n",
    "                                      'constant', constant_values=0)\n",
    "                        img = np.concatenate((img0, img1), axis=1)\n",
    "                        pl.figure()\n",
    "                        pl.imshow(img)\n",
    "                        cmap = pl.get_cmap('jet')\n",
    "                        for i in range(n_viz):\n",
    "                            (x0, y0), (x1, y1) = viz_matches_im_query[i].T, viz_matches_im_map[i].T\n",
    "                            pl.plot([x0, x1 + W0], [y0, y1], '-+',\n",
    "                                    color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)\n",
    "                        pl.show(block=True)\n",
    "\n",
    "                    valid_all = map_view['valid']\n",
    "                    pts3d = map_view['pts3d']\n",
    "\n",
    "                    WM_full, HM_full = map_view['rgb'].size\n",
    "                    map_rgb_tensor, map_K, map_to_orig_max, map_to_resize_max, (HM, WM) = resize_image_to_max(\n",
    "                        args.max_image_size, map_view['rgb'], map_view['intrinsics'])\n",
    "                    if WM_full != WM or HM_full != HM:\n",
    "                        y_full, x_full = torch.where(valid_all)\n",
    "                        pos2d_cv2 = torch.stack([x_full, y_full], dim=-1).cpu().numpy().astype(np.float64)\n",
    "                        sparse_pts3d = pts3d[y_full, x_full].cpu().numpy()\n",
    "                        _, _, pts3d_max, valid_max = rescale_points3d(\n",
    "                            pos2d_cv2, sparse_pts3d, map_to_resize_max, HM, WM)\n",
    "                        pts3d = torch.from_numpy(pts3d_max)\n",
    "                        valid_all = torch.from_numpy(valid_max)\n",
    "\n",
    "                    coarse_matches_im0 = geotrf(query_to_resize_max, coarse_matches_im0, norm=True)\n",
    "                    coarse_matches_im1 = geotrf(map_to_resize_max, coarse_matches_im1, norm=True)\n",
    "\n",
    "                    crops1, crops2 = [], []\n",
    "                    crops_v1, crops_p1 = [], []\n",
    "                    to_orig1, to_orig2 = [], []\n",
    "                    map_resolution = get_HW_resolution(HM, WM, maxdim=maxdim, patchsize=model.patch_embed.patch_size)\n",
    "\n",
    "                    for crop_q, crop_b, pair_tag in select_pairs_of_crops(map_rgb_tensor,\n",
    "                                                                          query_rgb_tensor,\n",
    "                                                                          coarse_matches_im1,\n",
    "                                                                          coarse_matches_im0,\n",
    "                                                                          maxdim=maxdim,\n",
    "                                                                          overlap=.5,\n",
    "                                                                          forced_resolution=[map_resolution,\n",
    "                                                                                             query_resolution]):\n",
    "                        # Per crop processing\n",
    "                        if not args.c2f_crop_with_homography:\n",
    "                            map_K = None\n",
    "                            query_K = None\n",
    "\n",
    "                        c1, v1, p1, trf1 = crop(map_rgb_tensor, valid_all, pts3d, crop_q, map_K)\n",
    "                        c2, _, _, trf2 = crop(query_rgb_tensor, None, None, crop_b, query_K)\n",
    "                        crops1.append(c1)\n",
    "                        crops2.append(c2)\n",
    "                        crops_v1.append(v1)\n",
    "                        crops_p1.append(p1)\n",
    "                        to_orig1.append(trf1)\n",
    "                        to_orig2.append(trf2)\n",
    "\n",
    "                    if len(crops1) == 0 or len(crops2) == 0:\n",
    "                        valid_pts3d, matches_im_query, matches_im_map, matches_conf = [], [], [], []\n",
    "                    else:\n",
    "                        crops1, crops2 = torch.stack(crops1), torch.stack(crops2)\n",
    "                        if len(crops1.shape) == 3:\n",
    "                            crops1, crops2 = crops1[None], crops2[None]\n",
    "                        crops_v1 = torch.stack(crops_v1)\n",
    "                        crops_p1 = torch.stack(crops_p1)\n",
    "                        to_orig1, to_orig2 = torch.stack(to_orig1), torch.stack(to_orig2)\n",
    "                        map_crop_view = dict(img=crops1.permute(0, 3, 1, 2),\n",
    "                                             instance=['1' for _ in range(crops1.shape[0])],\n",
    "                                             valid=crops_v1, pts3d=crops_p1,\n",
    "                                             to_orig=to_orig1)\n",
    "                        query_crop_view = dict(img=crops2.permute(0, 3, 1, 2),\n",
    "                                               instance=['2' for _ in range(crops2.shape[0])],\n",
    "                                               to_orig=to_orig2)\n",
    "\n",
    "                        # Inference and Matching\n",
    "                        valid_pts3d, matches_im_query, matches_im_map, matches_conf = fine_matching(query_crop_view,\n",
    "                                                                                                    map_crop_view,\n",
    "                                                                                                    model, device,\n",
    "                                                                                                    args.max_batch_size,\n",
    "                                                                                                    args.pixel_tol,\n",
    "                                                                                                    fast_nn_params)\n",
    "                        matches_im_query = geotrf(query_to_orig_max, matches_im_query, norm=True)\n",
    "                        matches_im_map = geotrf(map_to_orig_max, matches_im_map, norm=True)\n",
    "                else:\n",
    "                    # use only valid 2d points\n",
    "                    valid_pts3d, matches_im_query, matches_im_map, matches_conf = coarse_matching(query_view, map_view,\n",
    "                                                                                                  model, device,\n",
    "                                                                                                  args.pixel_tol,\n",
    "                                                                                                  fast_nn_params)\n",
    "                if cache_file is not None:\n",
    "                    mkdir_for(cache_file)\n",
    "                    np.savez(cache_file, valid_pts3d=valid_pts3d, matches_im_query=matches_im_query,\n",
    "                             matches_im_map=matches_im_map, matches_conf=matches_conf)\n",
    "\n",
    "            # apply conf\n",
    "            if len(matches_conf) > 0:\n",
    "                mask = matches_conf >= conf_thr\n",
    "                valid_pts3d = valid_pts3d[mask]\n",
    "                matches_im_query = matches_im_query[mask]\n",
    "                matches_im_map = matches_im_map[mask]\n",
    "                matches_conf = matches_conf[mask]\n",
    "\n",
    "            # visualize a few matches\n",
    "            if viz_matches > 0:\n",
    "                num_matches = matches_im_map.shape[0]\n",
    "                print(f'found {num_matches} matches')\n",
    "\n",
    "                viz_imgs = [np.array(query_view['rgb']), np.array(map_view['rgb'])]\n",
    "                from matplotlib import pyplot as pl\n",
    "                n_viz = viz_matches\n",
    "                match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)\n",
    "                viz_matches_im_query = matches_im_query[match_idx_to_viz]\n",
    "                viz_matches_im_map = matches_im_map[match_idx_to_viz]\n",
    "\n",
    "                H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]\n",
    "                img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
    "                img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
    "                img = np.concatenate((img0, img1), axis=1)\n",
    "                pl.figure()\n",
    "                pl.imshow(img)\n",
    "                cmap = pl.get_cmap('jet')\n",
    "                for i in range(n_viz):\n",
    "                    (x0, y0), (x1, y1) = viz_matches_im_query[i].T, viz_matches_im_map[i].T\n",
    "                    pl.plot([x0, x1 + W0], [y0, y1], '-+', color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)\n",
    "                pl.show(block=True)\n",
    "\n",
    "            if len(valid_pts3d) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                query_pts3d.append(valid_pts3d)\n",
    "                query_pts2d.append(matches_im_query)\n",
    "\n",
    "        if len(query_pts2d) == 0:\n",
    "            success = False\n",
    "            pr_querycam_to_world = None\n",
    "        else:\n",
    "            query_pts2d = np.concatenate(query_pts2d, axis=0).astype(np.float32)\n",
    "            query_pts3d = np.concatenate(query_pts3d, axis=0)\n",
    "            if len(query_pts2d) > pnp_max_points:\n",
    "                idxs = random.sample(range(len(query_pts2d)), pnp_max_points)\n",
    "                query_pts3d = query_pts3d[idxs]\n",
    "                query_pts2d = query_pts2d[idxs]\n",
    "\n",
    "            W, H = query_view['rgb'].size\n",
    "            if reprojection_error_diag_ratio is not None:\n",
    "                reprojection_error_img = reprojection_error_diag_ratio * math.sqrt(W**2 + H**2)\n",
    "            else:\n",
    "                reprojection_error_img = reprojection_error\n",
    "            success, pr_querycam_to_world = run_pnp(query_pts2d, query_pts3d,\n",
    "                                                    query_view['intrinsics'], query_view['distortion'],\n",
    "                                                    pnp_mode, reprojection_error_img, img_size=[W, H])\n",
    "\n",
    "        if not success:\n",
    "            abs_transl_error = float('inf')\n",
    "            abs_angular_error = float('inf')\n",
    "        else:\n",
    "            abs_transl_error, abs_angular_error = get_pose_error(pr_querycam_to_world, query_view['cam_to_world'])\n",
    "\n",
    "        pose_errors.append(abs_transl_error)\n",
    "        angular_errors.append(abs_angular_error)\n",
    "        poses_pred.append(pr_querycam_to_world)\n",
    "\n",
    "    xp_label = params_str + f'_conf_{conf_thr}'\n",
    "    if args.output_label:\n",
    "        xp_label = args.output_label + \"_\" + xp_label\n",
    "    if reprojection_error_diag_ratio is not None:\n",
    "        xp_label = xp_label + f'_reproj_diag_{reprojection_error_diag_ratio}'\n",
    "    else:\n",
    "        xp_label = xp_label + f'_reproj_err_{reprojection_error}'\n",
    "    export_results(args.output_dir, xp_label, query_names, poses_pred)\n",
    "    out_string = aggregate_stats(f'{args.dataset}', pose_errors, angular_errors)\n",
    "    print(out_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/bjangley/VPR/kapture/tools/kapture_merge.py -v debug -i /home/bjangley/VPR/mast3r/datasets/aachenv11/kapture/query_day /home/bjangley/VPR/mast3r/datasets/aachenv11/kapture/query_night -o /home/bjangley/VPR/mast3r/datasets/aachenv11/kapture/query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/bjangley/VPR/kapture/tools/kapture_import_colmap.py -v debug -txt /home/bjangley/VPR/mast3r/datasets/aachenv11/3D-models/aachen_v_1_1/ -im images -o /home/bjangley/VPR/mast3r/datasets/aachenv11/kapture/mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset \"VislocAachenDayNight('/home/bjangley/VPR/mast3r/datasets/aachenv11/', subscene='${all}', pairsfile='fire_top50', topk=20)\" --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /home/bjangley/VPR/mast3r/datasets/aachenv11/output/${all}/loc"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
